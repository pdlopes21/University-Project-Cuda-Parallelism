{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ffb9f877",
   "metadata": {},
   "source": [
    "## Criar dados no Colab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "114a72e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random as rd\n",
    "\n",
    "N = 100000\n",
    "FEATURES = 10\n",
    "\n",
    "cols = \"abcdefghijkmnopqrstuv\"\n",
    "columns = list(cols)[:FEATURES]\n",
    "\n",
    "x = np.random.rand(N, FEATURES)\n",
    "\n",
    "df = pd.DataFrame(x, columns = columns)\n",
    "df[\"y\"] = np.sin(df[\"a\"].values) + np.cos(df[\"b\"].values) + np.random.rand(N) * 0.001\n",
    "\n",
    "df.to_csv(\"data.csv\")\n",
    "\n",
    "\n",
    "unary_funs = [\"sinf\", \"cosf\", \"sqrtf\"]\n",
    "operators = [\"+\", \"-\"]\n",
    "\n",
    "def random_program(depth=4):\n",
    "    r = rd.randint(0,100)\n",
    "    if depth == 0 or r < 30:\n",
    "        c = rd.choice(columns)\n",
    "        return f\"_{c}_\"\n",
    "    elif r < 80:\n",
    "        c = rd.choice(unary_funs)\n",
    "        r = random_program(depth-1)\n",
    "        return f\"{c}({r})\"\n",
    "    else:\n",
    "        c = rd.choice(operators)\n",
    "        r1 = random_program(depth-1)\n",
    "        r2 = random_program(depth-1)\n",
    "        return f\"({r1}) {c} ({r2})\"\n",
    "\n",
    "\n",
    "with open(\"functions.txt\", \"w\") as f:\n",
    "    for _ in range(1000):\n",
    "        f.write(random_program() + \"\\n\")\n",
    "\n",
    "        \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5673bed5",
   "metadata": {},
   "source": [
    "## Versão Sequencial CPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12a4aed1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7980899860282042 _c_\n",
      "0.1451417934088247 (sinf(cosf(sinf(_d_)))) + (cosf(sinf(cosf(_b_))))\n",
      "Time: 0.01823878288269043\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<string>:1: RuntimeWarning: invalid value encountered in sqrt\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "\n",
    "df = pd.read_csv(\"data.csv\")\n",
    "\n",
    "funs = [ line.strip() for line in open(\"functions.txt\").readlines() ]\n",
    "\n",
    "def score(line):\n",
    "    for u in [\"sinf\", \"cosf\", \"tanf\", \"sqrtf\", \"expf\"]:\n",
    "        line = line.replace(u, f\"np.{u[:-1]}\")\n",
    "    for c in df.columns:\n",
    "        line = line.replace(f\"_{c}_\", f\"(df[\\\"{c}\\\"].values)\")\n",
    "    a = eval(line)\n",
    "    b = df[\"y\"]\n",
    "    e = np.square(np.subtract(a, b)).mean()\n",
    "    return e\n",
    "\n",
    "l = funs[0]\n",
    "print(score(l), l)\n",
    "\n",
    "# Calculate time\n",
    "sta = time.time()\n",
    "r = min([ (score(line), line) for line in funs ])\n",
    "end = time.time()\n",
    "print(f\"{r[0]} {r[1]}\")\n",
    "print(\"Time:\", end-sta)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0ad6783",
   "metadata": {},
   "source": [
    "## VERSÃO GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b19e3e9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thu Dec 11 17:12:34 2025       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  Tesla T4                       Off |   00000000:00:04.0 Off |                    0 |\n",
      "| N/A   61C    P8              9W /   70W |       0MiB /  15360MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|  No running processes found                                                             |\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "036b474d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ptxas warning : Stack size for entry function '_Z22compute_squared_errorsPKcPKfS2_iiPf' cannot be statically determined\n",
      "Teste GPU\n",
      "\n",
      "Results\n",
      "First function MSE: 0.7980899215 _c_\n",
      "Best function MSE: 0.1451418102 (sinf(cosf(sinf(_d_)))) + (cosf(sinf(cosf(_b_))))\n",
      "GPU kernel execution time: 1.536 ms\n",
      "Total execution time (including data transfer): 18.725 ms\n"
     ]
    }
   ],
   "source": [
    "#CORRE CÓDIGO ABAIXO, METI AQUI EM CIMA PARA NAO TER QUE\n",
    "#DAR SCROLL CONSTANTEMENTE\n",
    "!nvcc -arch=sm_75 gpuProj4.cu -o gpuProj4\n",
    "!./gpuProj4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2c49ee65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing gpuProj4.cu\n"
     ]
    }
   ],
   "source": [
    "%%writefile gpuProj4.cu\n",
    "\n",
    "//ESTA CLASSE É IGUAL AO DO JUPYTER\n",
    "//SERVE PARA SER MAIS FÁCIL DE ALTERAR E VISUALIZAR\n",
    "\n",
    "\n",
    "#include <cuda_runtime.h>\n",
    "#include <stdio.h>\n",
    "#include <stdlib.h>\n",
    "#include <string.h>\n",
    "#include <float.h>\n",
    "#include <math.h>\n",
    "\n",
    "//Tentei usar __constant__, mas dava problema com a inicialização dos arrays\n",
    "#define FEATURES 10\n",
    "#define MAX_EXPR_LEN 200\n",
    "#define MAX_DATA_ROWS 100000\n",
    "#define MAX_FUNCTIONS 1000\n",
    "#define THREADS_PER_BLOCK 256\n",
    "\n",
    "\n",
    "//Não percebo o porquê mas isto é necessário na compilação\n",
    "__device__ float parse_expr(const char* expr, int* pos, const float* features);\n",
    "__device__ float parse_factor(const char* expr, int* pos, const float* features);\n",
    "\n",
    "\n",
    "\n",
    "__host__ int read_data(float *data, float *targets) {\n",
    "    FILE *file = fopen(\"data.csv\", \"r\");\n",
    "    if (file == NULL) {\n",
    "        printf(\"Error opening data.csv\\n\");\n",
    "        exit(1);\n",
    "    }\n",
    "    char line[1024];\n",
    "    fgets(line, sizeof(line), file); // Skip header\n",
    "    \n",
    "    int num_rows = 0;\n",
    "    while (fgets(line, sizeof(line), file) && num_rows < MAX_DATA_ROWS) {\n",
    "        char *token = strtok(line, \",\");\n",
    "        token = strtok(NULL, \",\"); // Skip index\n",
    "        \n",
    "        for (int j = 0; j < FEATURES; j++) {\n",
    "            if (token != NULL) {\n",
    "                data[num_rows * FEATURES + j] = atof(token);\n",
    "                token = strtok(NULL, \",\");\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        if (token != NULL) {\n",
    "            targets[num_rows] = atof(token);\n",
    "        }\n",
    "        \n",
    "        num_rows++;\n",
    "    }\n",
    "    fclose(file);\n",
    "    return num_rows;\n",
    "}\n",
    "\n",
    "\n",
    "__host__ int read_functions(char* functions) {\n",
    "    FILE* fp = fopen(\"functions.txt\", \"r\");\n",
    "    if (!fp) {\n",
    "        printf(\"Error opening functions.txt\\n\");\n",
    "        return 0;\n",
    "    }\n",
    "    \n",
    "    int count = 0;\n",
    "    char line[MAX_EXPR_LEN];\n",
    "    \n",
    "    while (fgets(line, sizeof(line), fp) && count < MAX_FUNCTIONS) {\n",
    "        //Remove newline\n",
    "        line[strcspn(line, \"\\n\")] = 0;\n",
    "        strncpy(functions + count * MAX_EXPR_LEN, line, MAX_EXPR_LEN - 1);\n",
    "        functions[count * MAX_EXPR_LEN + MAX_EXPR_LEN - 1] = '\\0';\n",
    "        count++;\n",
    "    }\n",
    "    \n",
    "    fclose(fp);\n",
    "    return count;\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "__global__ void compute_squared_errors(const char* functions, const float* __restrict__ data, const float* __restrict__ targets, \n",
    "                                        int num_rows, int num_functions, float* __restrict__ errors) {\n",
    "    int data_idx = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "    int func_idx = blockIdx.y;\n",
    "    \n",
    "    if (data_idx >= num_rows || func_idx >= num_functions) {\n",
    "        return;\n",
    "    }\n",
    "    \n",
    "    //Menos problemas que arrays\n",
    "    const char* expr = functions + func_idx * MAX_EXPR_LEN;\n",
    "    const float* features = data + data_idx * FEATURES;\n",
    "    \n",
    "    int pos = 0;\n",
    "    float predicted = parse_expr(expr, &pos, features);\n",
    "    float error = predicted - targets[data_idx];\n",
    "    \n",
    "    //Erros são guardados já elevados ao quadrado,\n",
    "\t//distribui melhor a carga de trabalho\n",
    "    errors[func_idx * num_rows + data_idx] = error * error;\n",
    "}\n",
    "\n",
    "\n",
    "__device__ void skip_whitespace(const char* expr, int* pos) {\n",
    "    while (expr[*pos] == ' ') (*pos)++;\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "//Seccão de parsing feita com AI Claude, por mais que tentasse não consegui meter esta parte funcional sem AI\n",
    "//Dificil de seguir logica de cabeça, aconselhado a acompanhar com desenho no papel\n",
    "//Este método garante que a precencia de parênteses é corretamente tratada\n",
    "\n",
    "//Adições e subtrações\n",
    "__device__ float parse_expr(const char* expr, int* pos, const float* features) {\n",
    "    float result = parse_factor(expr, pos, features);\n",
    "    \n",
    "    while (1) {\n",
    "        skip_whitespace(expr, pos);\n",
    "        \n",
    "        if (expr[*pos] == '+') {\n",
    "            (*pos)++;\n",
    "            result += parse_factor(expr, pos, features);\n",
    "        } else if (expr[*pos] == '-') {\n",
    "            (*pos)++;\n",
    "            result -= parse_factor(expr, pos, features);\n",
    "        } else {\n",
    "            break;\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    return result;\n",
    "}\n",
    "\n",
    "//Não existem multiplicações nem divisões no enunciado\n",
    "\n",
    "//Parenteses, trignometria, e raizes (aka operaçoes com prioridade)\n",
    "__device__ float parse_factor(const char* expr, int* pos, const float* features) {\n",
    "    skip_whitespace(expr, pos);\n",
    "    \n",
    "    // Check for opening parenthesis\n",
    "    if (expr[*pos] == '(') {\n",
    "        (*pos)++; // consume '('\n",
    "        float result = parse_expr(expr, pos, features);\n",
    "        skip_whitespace(expr, pos);\n",
    "        if (expr[*pos] == ')') {\n",
    "\t\t\t (*pos)++; // consume ')'\n",
    "\t\t}\n",
    "        return result;\n",
    "    }\n",
    "    \n",
    "    if (expr[*pos] == 's' && expr[*pos+1] == 'i' && expr[*pos+2] == 'n' && expr[*pos+3] == 'f') {\n",
    "        *pos += 4;\n",
    "        skip_whitespace(expr, pos);\n",
    "        if (expr[*pos] == '(') {\n",
    "\t\t\t (*pos)++;\n",
    "\t\t}\n",
    "        float arg = parse_expr(expr, pos, features);\n",
    "        skip_whitespace(expr, pos);\n",
    "        if (expr[*pos] == ')') {\n",
    "\t\t\t (*pos)++;\n",
    "\t\t}\n",
    "        return sinf(arg);\n",
    "    }\n",
    "    \n",
    "    if (expr[*pos] == 'c' && expr[*pos+1] == 'o' && expr[*pos+2] == 's' && expr[*pos+3] == 'f') {\n",
    "        *pos += 4;\n",
    "        skip_whitespace(expr, pos);\n",
    "        if (expr[*pos] == '(') {\n",
    "\t\t\t (*pos)++;\n",
    "\t\t}\n",
    "        float arg = parse_expr(expr, pos, features);\n",
    "        skip_whitespace(expr, pos);\n",
    "        if (expr[*pos] == ')') {\n",
    "\t\t\t (*pos)++;\n",
    "\t\t}\n",
    "        return cosf(arg);\n",
    "    }\n",
    "    \n",
    "    if (expr[*pos] == 's' && expr[*pos+1] == 'q' && expr[*pos+2] == 'r' && expr[*pos+3] == 't' && expr[*pos+4] == 'f') {\n",
    "        *pos += 5;\n",
    "        skip_whitespace(expr, pos);\n",
    "        if (expr[*pos] == '(') {\n",
    "\t\t\t (*pos)++;\n",
    "\t\t}\n",
    "        float arg = parse_expr(expr, pos, features);\n",
    "        skip_whitespace(expr, pos);\n",
    "        if (expr[*pos] == ')') {\n",
    "\t\t\t (*pos)++;\n",
    "\t\t}\n",
    "        return sqrtf(arg);\n",
    "    }\n",
    "    \n",
    "    if (expr[*pos] == '_') {\n",
    "        (*pos)++;\n",
    "        char var = expr[*pos];\n",
    "        (*pos)++;\n",
    "        if (expr[*pos] == '_') {\n",
    "\t\t\t (*pos)++;\n",
    "\t\t}\n",
    "        \n",
    "        if (var >= 'a' && var <= 'j') {\n",
    "            return features[var - 'a'];\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    return 0.0f;\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "__global__ void reduce_to_mse(const float* errors, int num_rows, int num_functions, float* mse) {\n",
    "    \n",
    "\n",
    "    __shared__ float shared_sum[THREADS_PER_BLOCK];\n",
    "    \n",
    "    int func_idx = blockIdx.x; \n",
    "    int tid = threadIdx.x;\n",
    "    \n",
    "    if (func_idx >= num_functions) {\n",
    "        return;\n",
    "    }\n",
    "    \n",
    "    //Usa stride\n",
    "    float sum = 0.0f;\n",
    "    for (int i = tid; i < num_rows; i += blockDim.x) {\n",
    "        sum += errors[func_idx * num_rows + i];\n",
    "    }\n",
    "    \n",
    "   \n",
    "    shared_sum[tid] = sum;\n",
    "    __syncthreads(); \n",
    "    \n",
    "    // Tree-based reduction:\n",
    "    //Ver gráfico no youtube: \"CUDA Crash Course: Sum Reduction Part 1\" aos aos 54 segundos\n",
    "    for (int stride = blockDim.x / 2; stride > 0; stride = stride / 2) {\n",
    "        if (tid < stride) {\n",
    "            shared_sum[tid] += shared_sum[tid + stride];\n",
    "        }\n",
    "        __syncthreads();  //Necessário esperar para executar cada passo\n",
    "    }\n",
    "    \n",
    "    \n",
    "    if (tid == 0) {\n",
    "        mse[func_idx] = shared_sum[0] / num_rows;\n",
    "    }\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "int main() {\n",
    "    printf(\"Teste GPU\\n\\n\");\n",
    "\n",
    "    //tempo total\n",
    "    //Tive problemas a confirmar que o tempo está a ser medido corretamente\n",
    "    //Aconselho o professor a verificar este ponto e ver se está tudo correto\n",
    "    cudaEvent_t total_start, total_stop;\n",
    "    cudaEventCreate(&total_start);\n",
    "    cudaEventCreate(&total_stop);\n",
    "    cudaEventRecord(total_start);\n",
    "    \n",
    "    //Malloc para o CPU\n",
    "    float *h_data = (float *)malloc(MAX_DATA_ROWS * FEATURES * sizeof(float));\n",
    "    float *h_targets = (float *)malloc(MAX_DATA_ROWS * sizeof(float));\n",
    "    char *h_functions = (char *)malloc(MAX_FUNCTIONS * MAX_EXPR_LEN * sizeof(char));\n",
    "    float *h_mse = (float *)malloc(MAX_FUNCTIONS * sizeof(float));\n",
    "    \n",
    "    if (!h_data || !h_targets || !h_functions || !h_mse) {\n",
    "        printf(\"Failed to allocate host memory\\n\");\n",
    "        return 1;\n",
    "    }\n",
    "    \n",
    "    //Ints necessário para quando o generate_inputs é alterado\n",
    "    int num_rows = read_data(h_data, h_targets);\n",
    "    int num_functions = read_functions(h_functions);\n",
    "    \n",
    "    //Malloc para a GPU\n",
    "    float *d_data, *d_targets, *d_errors;\n",
    "    char *d_functions;\n",
    "    \n",
    "    size_t data_size = num_rows * FEATURES * sizeof(float);\n",
    "    size_t targets_size = num_rows * sizeof(float);\n",
    "    size_t functions_size = num_functions * MAX_EXPR_LEN * sizeof(char);\n",
    "    size_t errors_size = num_functions * num_rows * sizeof(float);\n",
    "    size_t mse_size = num_functions * sizeof(float);\n",
    "    \n",
    "    cudaMalloc(&d_data, data_size);\n",
    "    cudaMalloc(&d_targets, targets_size);\n",
    "    cudaMalloc(&d_functions, functions_size);\n",
    "    cudaMalloc(&d_errors, errors_size);\n",
    "    //Metodo antigo de alocar o mse na GPU\n",
    "    //cudaMalloc(&d_mse, mse_size);\n",
    "    //(Nota 1) mse passou a ser allocado para memoria partilhada CPU/GPU\n",
    "\t//É o unico dado que tem que ser devolvido para a CPU\n",
    "    float *mse;\n",
    "    cudaMallocManaged(&mse, mse_size);\n",
    "    \n",
    "    \n",
    "    //Copiar dados partilhados (usados por todas as funções) antes\n",
    "    //Versão anterior incluia copia de funcoes, que agora é feita em streams\n",
    "    //Como as funcoes precisam de todos os dados, estes têm que ser copiados antes\n",
    "    cudaMemcpy(d_data, h_data, data_size, cudaMemcpyHostToDevice);\n",
    "    cudaMemcpy(d_targets, h_targets, targets_size, cudaMemcpyHostToDevice);\n",
    "    \n",
    "    //Evento para medir o tempo de execução, prenda do Claude\n",
    "    cudaEvent_t start, stop;\n",
    "    cudaEventCreate(&start);\n",
    "    cudaEventCreate(&stop);\n",
    "\n",
    "    cudaEventRecord(start);\n",
    "    \n",
    "    //Criar múltiplas streams para task parallelism\n",
    "    const int NUM_STREAMS = 4;\n",
    "    cudaStream_t streams[NUM_STREAMS];\n",
    "    for (int i = 0; i < NUM_STREAMS; i++) {\n",
    "        cudaStreamCreate(&streams[i]);\n",
    "    }\n",
    "    \n",
    "    //x Para cada ponto, Y para cada funções\n",
    "    dim3 threadsPerBlock(THREADS_PER_BLOCK, 1);\n",
    "    //Implementacao antiga defenia blocks per grid aqui\n",
    "    //Streams obrigam a definir mais tarde caso o numero de funcoes mude\n",
    "    \n",
    "    //Dividir funções entre streams\n",
    "    int funcs_per_stream = (num_functions + NUM_STREAMS - 1) / NUM_STREAMS;\n",
    "\n",
    "    \n",
    "    //Nova versão com streams\n",
    "    for (int s = 0; s < NUM_STREAMS; s++) {\n",
    "        int start_func = s * funcs_per_stream;\n",
    "        \n",
    "        int end_func;\n",
    "        if(start_func + funcs_per_stream < num_functions) {\n",
    "            end_func = start_func + funcs_per_stream;\n",
    "        } else {\n",
    "            end_func = num_functions;\n",
    "        }\n",
    "\n",
    "        int funcs_in_stream = end_func - start_func;\n",
    "        \n",
    "        if (funcs_in_stream <= 0) continue;\n",
    "        \n",
    "        //Copiar apenas as funções necessárias para esta stream \n",
    "        //Foi aconselhado fazer async\n",
    "        size_t func_chunk_size = funcs_in_stream * MAX_EXPR_LEN * sizeof(char);\n",
    "        cudaMemcpyAsync(d_functions + start_func * MAX_EXPR_LEN, \n",
    "                        h_functions + start_func * MAX_EXPR_LEN,\n",
    "                        func_chunk_size, \n",
    "                        cudaMemcpyHostToDevice, \n",
    "                        streams[s]);\n",
    "        \n",
    "        dim3 blocksPerGrid((num_rows + THREADS_PER_BLOCK - 1) / THREADS_PER_BLOCK, funcs_in_stream);\n",
    "        compute_squared_errors<<<blocksPerGrid, threadsPerBlock, 0, streams[s]>>>(\n",
    "            d_functions + start_func * MAX_EXPR_LEN, \n",
    "            d_data, d_targets, \n",
    "            num_rows, funcs_in_stream, \n",
    "            d_errors + start_func * num_rows);\n",
    "        \n",
    "        reduce_to_mse<<<funcs_in_stream, THREADS_PER_BLOCK, 0, streams[s]>>>(\n",
    "            d_errors + start_func * num_rows, \n",
    "            num_rows, funcs_in_stream, \n",
    "            mse + start_func);\n",
    "    }\n",
    "    \n",
    "    //Sincronizar todas as streams\n",
    "    for (int i = 0; i < NUM_STREAMS; i++) {\n",
    "        cudaStreamSynchronize(streams[i]);\n",
    "    }\n",
    "\n",
    "    //Stop não para imediatamente a temporização\n",
    "    //Em vez disso, mete o evento \"paragem de cronometragem\" em queue para quando todos os\n",
    "    //kernels anteriores estiverem completos\n",
    "    cudaEventRecord(stop);  \n",
    "    \n",
    "    //Metodo antigo de copiar o mse para a CPU\n",
    "    //cudaMemcpy(h_mse, d_mse, mse_size, cudaMemcpyDeviceToHost);\n",
    "    //(Nota 1) Se usar cudaMallocManaged, não é necessário este memcpy\n",
    "\n",
    "\n",
    "    //Necessário esperar antes de ler\n",
    "    cudaDeviceSynchronize();\n",
    "\n",
    "    \n",
    "    float milliseconds = 0;\n",
    "    cudaEventElapsedTime(&milliseconds, start, stop);\n",
    "    \n",
    "    \n",
    "    int min_idx = 0;\n",
    "    float min_mse = mse[0];\n",
    "    for (int i = 1; i < num_functions; i++) {\n",
    "        if (mse[i] < min_mse) {\n",
    "            min_mse = mse[i];\n",
    "            min_idx = i;\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    //stop tempo total\n",
    "    cudaEventRecord(total_stop);\n",
    "    cudaEventSynchronize(total_stop);\n",
    "    \n",
    "    float total_milliseconds = 0;\n",
    "    cudaEventElapsedTime(&total_milliseconds, total_start, total_stop);\n",
    "\n",
    "    //Alguma luta para perceber prints em C até hoje, resumo para a posteridade após pesquisa\n",
    "    //% chama-se format specifier\n",
    "    //Descobrir o nome dos % foi mais dificil que muitos projetos da faculdade\n",
    "    //f no final é para indicar um float\n",
    "    //.10 e .3 indicam o número de casas decimais após a vírgula\n",
    "    //default é 6\n",
    "    //Se tivesse um int em vez dum float, .10 indicaria o numero de digitos a serem mostrados\n",
    "    //s é para strings, mas esse até eu sabia\n",
    "    printf(\"Results\\n\");\n",
    "    printf(\"First function MSE: %.10f %s\\n\", mse[0], h_functions);\n",
    "    printf(\"Best function MSE: %.10f %s\\n\", min_mse, h_functions + min_idx * MAX_EXPR_LEN);\n",
    "    printf(\"GPU kernel execution time: %.3f ms\\n\", milliseconds);\n",
    "    printf(\"Total execution time (including data transfer): %.3f ms\\n\", total_milliseconds);\n",
    "\n",
    "    \n",
    "    \n",
    "    cudaFree(d_data);\n",
    "    cudaFree(d_targets);\n",
    "    cudaFree(d_functions);\n",
    "    cudaFree(d_errors);\n",
    "    //Versão antiga\n",
    "    //cudaFree(d_mse);\n",
    "    \n",
    "    //Pelos vistos é \"necessário\" destruir as streams\n",
    "    for (int i = 0; i < NUM_STREAMS; i++) {\n",
    "        cudaStreamDestroy(streams[i]);\n",
    "    }\n",
    "    \n",
    "    cudaEventDestroy(start);\n",
    "    cudaEventDestroy(stop);\n",
    "    cudaEventDestroy(total_start);\n",
    "    cudaEventDestroy(total_stop);\n",
    "    \n",
    "    free(h_data);\n",
    "    free(h_targets);\n",
    "    free(h_functions);\n",
    "    free(h_mse);\n",
    "\t\n",
    "\tcudaFree(mse);\n",
    "    \n",
    "    return 0;\n",
    "}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
